{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "#we read the training data\n",
    "df = pd.read_csv('../training/training_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>sentence</th>\n",
       "      <th>difficulty</th>\n",
       "      <th>n_words</th>\n",
       "      <th>avg_word_length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Les coûts kilométriques réels peuvent diverger...</td>\n",
       "      <td>C1</td>\n",
       "      <td>38</td>\n",
       "      <td>5.736842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Le bleu, c'est ma couleur préférée mais je n'a...</td>\n",
       "      <td>A1</td>\n",
       "      <td>12</td>\n",
       "      <td>4.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Le test de niveau en français est sur le site ...</td>\n",
       "      <td>A1</td>\n",
       "      <td>13</td>\n",
       "      <td>4.153846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Est-ce que ton mari est aussi de Boston?</td>\n",
       "      <td>A1</td>\n",
       "      <td>8</td>\n",
       "      <td>4.125000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Dans les écoles de commerce, dans les couloirs...</td>\n",
       "      <td>B1</td>\n",
       "      <td>34</td>\n",
       "      <td>5.176471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4795</th>\n",
       "      <td>4795</td>\n",
       "      <td>C'est pourquoi, il décida de remplacer les hab...</td>\n",
       "      <td>B2</td>\n",
       "      <td>26</td>\n",
       "      <td>5.384615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4796</th>\n",
       "      <td>4796</td>\n",
       "      <td>Il avait une de ces pâleurs splendides qui don...</td>\n",
       "      <td>C1</td>\n",
       "      <td>21</td>\n",
       "      <td>4.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4797</th>\n",
       "      <td>4797</td>\n",
       "      <td>Et le premier samedi de chaque mois, venez ren...</td>\n",
       "      <td>A2</td>\n",
       "      <td>14</td>\n",
       "      <td>4.785714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4798</th>\n",
       "      <td>4798</td>\n",
       "      <td>Les coûts liés à la journalisation n'étant pas...</td>\n",
       "      <td>C2</td>\n",
       "      <td>32</td>\n",
       "      <td>6.093750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4799</th>\n",
       "      <td>4799</td>\n",
       "      <td>Sur le sable, la mer haletait de toute la resp...</td>\n",
       "      <td>C2</td>\n",
       "      <td>17</td>\n",
       "      <td>4.647059</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4800 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                           sentence difficulty  \\\n",
       "0        0  Les coûts kilométriques réels peuvent diverger...         C1   \n",
       "1        1  Le bleu, c'est ma couleur préférée mais je n'a...         A1   \n",
       "2        2  Le test de niveau en français est sur le site ...         A1   \n",
       "3        3           Est-ce que ton mari est aussi de Boston?         A1   \n",
       "4        4  Dans les écoles de commerce, dans les couloirs...         B1   \n",
       "...    ...                                                ...        ...   \n",
       "4795  4795  C'est pourquoi, il décida de remplacer les hab...         B2   \n",
       "4796  4796  Il avait une de ces pâleurs splendides qui don...         C1   \n",
       "4797  4797  Et le premier samedi de chaque mois, venez ren...         A2   \n",
       "4798  4798  Les coûts liés à la journalisation n'étant pas...         C2   \n",
       "4799  4799  Sur le sable, la mer haletait de toute la resp...         C2   \n",
       "\n",
       "      n_words  avg_word_length  \n",
       "0          38         5.736842  \n",
       "1          12         4.250000  \n",
       "2          13         4.153846  \n",
       "3           8         4.125000  \n",
       "4          34         5.176471  \n",
       "...       ...              ...  \n",
       "4795       26         5.384615  \n",
       "4796       21         4.666667  \n",
       "4797       14         4.785714  \n",
       "4798       32         6.093750  \n",
       "4799       17         4.647059  \n",
       "\n",
       "[4800 rows x 5 columns]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Drop rows with missing 'sentence' or 'difficulty' in training data\n",
    "df = df.dropna(subset=['sentence', 'difficulty'])\n",
    "# remove duplicates\n",
    "df = df.drop_duplicates(subset=['sentence'])\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "#add a column with the number of words in the text ('sentence' column)\n",
    "df['n_words'] = df['sentence'].apply(lambda x: len(x.split()))\n",
    "#add a column with the average length of the words in the text ('sentence' column)\n",
    "df['avg_word_length'] = df['sentence'].apply(lambda x: np.mean([len(w) for w in x.split()]))\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "difficulty\n",
      "A1    813\n",
      "A2    795\n",
      "B1    795\n",
      "B2    792\n",
      "C1    798\n",
      "C2    807\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# we first want to understand the features of our dataset, so we will see how many sentences are available per level of difficulty\n",
    "print(df.groupby('difficulty').size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing of the target variable\n",
    "## 1. Label encoding of the target variable\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label mapping: {'A1': 0, 'A2': 1, 'B1': 2, 'B2': 3, 'C1': 4, 'C2': 5}\n",
      "['C1' 'A1' 'A1' ... 'A2' 'C2' 'C2'] [4 0 0 ... 1 5 5]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "y = df['difficulty'].values\n",
    "\n",
    "# Define the order of your labels\n",
    "labels_ordered = ['A1', 'A2', 'B1', 'B2', 'C1', 'C2']\n",
    "\n",
    "# Initialize the LabelEncoder\n",
    "encoder = LabelEncoder()\n",
    "\n",
    "# Manually fit the encoder to the ordered labels\n",
    "encoder.fit(labels_ordered)\n",
    "\n",
    "# Encode your actual labels\n",
    "y_encoded = encoder.transform(y)\n",
    "\n",
    "# Output the encoding to verify\n",
    "label_mapping = dict(zip(encoder.classes_, encoder.transform(encoder.classes_)))\n",
    "print(\"Label mapping:\", label_mapping)\n",
    "print(y, y_encoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing of the features\n",
    "## 1. Tokenization:\n",
    "Tokenization is the first step. It consists in breaking down each sentence into individual words or subwords. For french we can use a CamemBERT tokenizer or a FlauBert tokenizer.\n",
    "\n",
    "\n",
    "NB: FlauBERT: Uses </w> to denote the end of a word. This is typical of tokenizers that use subword segmentation like SentencePiece or BPE (Byte Pair Encoding) that FlauBERT employs. This method helps in handling unknown words better by breaking down words into more frequently occurring subwords.\n",
    "CamemBERT: Uses a special character (▁, an underscore) to denote the beginning of a new word. This is common in tokenizers that are designed to handle languages where whitespace alone isn't a reliable separator of words.\n",
    "## 2. Attention Masking, Padding and Truncation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Some weights of CamembertForSequenceClassification were not initialized from the model checkpoint at camembert-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['▁Ceci', '▁est', '▁une', '▁phrase', '▁française', '.']\n"
     ]
    }
   ],
   "source": [
    "from transformers import CamembertTokenizer, CamembertForSequenceClassification, FlaubertTokenizer, FlaubertForSequenceClassification\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch\n",
    "# 1) tokenization > used to encode the sentences\n",
    "# we could do the tokenization either with Camembert or Flaubert\n",
    "num_classes = df['difficulty'].nunique()\n",
    "chosen_tokenizer = 'camembert'\n",
    "\n",
    "if chosen_tokenizer == 'camembert':\n",
    "    tokenizer = CamembertTokenizer.from_pretrained('camembert-base')\n",
    "    model = CamembertForSequenceClassification.from_pretrained('camembert-base', num_labels=num_classes)\n",
    "elif chosen_tokenizer == 'flaubert':\n",
    "    tokenizer = FlaubertTokenizer.from_pretrained('flaubert/flaubert_base_cased')\n",
    "    model = FlaubertForSequenceClassification.from_pretrained('flaubert/flaubert_base_cased', num_labels=num_classes)\n",
    "\n",
    "tokens = tokenizer.tokenize(\"Ceci est une phrase française.\")\n",
    "print(tokens)\n",
    "\n",
    "# now we can proceed with the tokenization of the sentences\n",
    "#the dataclass handles the tokenization of sentences. it also uses the tokenizer to convert the tokens into \n",
    "# input ids (sequences of ints that uniquely identify each token in the vocabulary)\n",
    "# and attention masks (sequences of 1s and 0s that indicate which tokens should be attended to and which should not)\n",
    "# the dataclass also handles padding and truncation of the sentences (most ml models require consistent size)\n",
    "# it also handles labels when available so that it can be used for training and validation\n",
    "class CEFRDataset(Dataset):\n",
    "    def __init__(self, texts, tokenizer, max_len, labels=None):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            return_token_type_ids=False,\n",
    "            pad_to_max_length=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt',\n",
    "        )\n",
    "        item = {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten()\n",
    "        }\n",
    "        # Add labels to your dataset only if they are provided, i.e., during training and validation.\n",
    "        if self.labels is not None:\n",
    "            label = self.labels[idx]\n",
    "            item['labels'] = torch.tensor(label, dtype=torch.long)\n",
    "        return item\n",
    "\n",
    "\n",
    "dataset = CEFRDataset(df['sentence'], tokenizer, max_len=128, labels=y_encoded)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model\n",
    "### We do k-fold cross validation to find the best hyperparameters for the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Training with lr=1e-05, fold 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of CamembertForSequenceClassification were not initialized from the model checkpoint at camembert-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Epoch 1 Training: 100%|██████████| 240/240 [09:49<00:00,  2.46s/it]\n",
      "Epoch 2 Training: 100%|██████████| 240/240 [09:44<00:00,  2.44s/it]\n",
      "Epoch 3 Training: 100%|██████████| 240/240 [09:59<00:00,  2.50s/it]\n",
      "Epoch 4 Training: 100%|██████████| 240/240 [07:28<00:00,  1.87s/it]\n",
      "Epoch 5 Training: 100%|██████████| 240/240 [06:54<00:00,  1.73s/it]\n",
      "Validating: 100%|██████████| 60/60 [00:25<00:00,  2.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 validation accuracy: 0.5489583333333333\n",
      "Training with lr=1e-05, fold 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of CamembertForSequenceClassification were not initialized from the model checkpoint at camembert-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Epoch 1 Training: 100%|██████████| 240/240 [06:50<00:00,  1.71s/it]\n",
      "Epoch 2 Training: 100%|██████████| 240/240 [06:49<00:00,  1.71s/it]\n",
      "Epoch 3 Training:  40%|███▉      | 95/240 [07:48<04:40,  1.93s/it]  "
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "#split the dataset into training and validation datasets (in the future we will do a k-fold cross validation)\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "# Define number of folds\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using {device} device\")\n",
    "model.to(device)  # Move model to GPU if available\n",
    "\n",
    "num_epochs = 5\n",
    "\n",
    "\n",
    "n_splits = 5\n",
    "kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "\n",
    "import torch\n",
    "import tqdm\n",
    "from transformers import AdamW, get_scheduler\n",
    "\n",
    "# Define your learning rate and other hyperparameters\n",
    "learning_rates = [1e-5, 5e-5, 1e-4]  # Example learning rates to test\n",
    "\n",
    "# Initialize results container for hyperparameter tuning\n",
    "hyperparam_results = {}\n",
    "\n",
    "for lr in learning_rates:\n",
    "    fold_results = []\n",
    "    for fold, (train_idx, val_idx) in enumerate(kf.split(df)):\n",
    "        print(f\"Training with lr={lr}, fold {fold+1}/{n_splits}\")\n",
    "\n",
    "        # Split data according to current fold\n",
    "        train_sentences = df.iloc[train_idx]['sentence'].reset_index(drop=True)\n",
    "        train_labels = y_encoded[train_idx]\n",
    "        val_sentences = df.iloc[val_idx]['sentence'].reset_index(drop=True)\n",
    "        val_labels = y_encoded[val_idx]\n",
    "\n",
    "        # Create datasets\n",
    "        train_dataset = CEFRDataset(train_sentences, tokenizer, max_len=128, labels=train_labels)\n",
    "        val_dataset = CEFRDataset(val_sentences, tokenizer, max_len=128, labels=val_labels)\n",
    "\n",
    "        # Create data loaders\n",
    "        train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "        val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)\n",
    "\n",
    "        # Initialize model and move it to the device\n",
    "        model = CamembertForSequenceClassification.from_pretrained('camembert-base', num_labels=len(np.unique(y_encoded)))\n",
    "        model.to(device)\n",
    "\n",
    "        # Initialize optimizer and scheduler\n",
    "        optimizer = AdamW(model.parameters(), lr=lr)\n",
    "        num_training_steps = num_epochs * len(train_loader)\n",
    "        lr_scheduler = get_scheduler(\"linear\", optimizer=optimizer, num_warmup_steps=0, num_training_steps=num_training_steps)\n",
    "\n",
    "        # Training loop\n",
    "        model.train()\n",
    "        for epoch in range(num_epochs):\n",
    "            for batch in tqdm.tqdm(train_loader, desc=f\"Epoch {epoch + 1} Training\"):\n",
    "                batch = {k: v.to(device) for k, v in batch.items()}\n",
    "                outputs = model(**batch)\n",
    "                loss = outputs.loss\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                lr_scheduler.step()\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "        # Validation loop\n",
    "        total_eval_accuracy = 0\n",
    "        model.eval()\n",
    "        for batch in tqdm.tqdm(val_loader, desc=\"Validating\"):\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            with torch.no_grad():\n",
    "                outputs = model(**batch)\n",
    "            logits = outputs.logits\n",
    "            predictions = torch.argmax(logits, dim=1)\n",
    "            accuracy = (predictions == batch['labels']).float().mean()\n",
    "            total_eval_accuracy += accuracy.item()\n",
    "\n",
    "        avg_val_accuracy = total_eval_accuracy / len(val_loader)\n",
    "        fold_results.append(avg_val_accuracy)\n",
    "        print(f\"Fold {fold+1} validation accuracy: {avg_val_accuracy}\")\n",
    "\n",
    "    # Average accuracy across folds for current learning rate\n",
    "    average_accuracy = sum(fold_results) / len(fold_results)\n",
    "    hyperparam_results[lr] = average_accuracy\n",
    "    print(f\"Average validation accuracy with lr={lr}: {average_accuracy}\")\n",
    "\n",
    "# Identify best performing hyperparameters\n",
    "best_lr = max(hyperparam_results, key=hyperparam_results.get)\n",
    "print(f\"Best learning rate: {best_lr} with an average validation accuracy of {hyperparam_results[best_lr]}\")\n",
    "\n",
    "# hyperparameter tuning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.71      0.77        91\n",
      "           1       0.48      0.61      0.54        72\n",
      "           2       0.63      0.52      0.57        92\n",
      "           3       0.44      0.59      0.50        70\n",
      "           4       0.48      0.42      0.45        71\n",
      "           5       0.67      0.63      0.65        84\n",
      "\n",
      "    accuracy                           0.59       480\n",
      "   macro avg       0.59      0.58      0.58       480\n",
      "weighted avg       0.60      0.59      0.59       480\n",
      "\n",
      "[[65 24  2  0  0  0]\n",
      " [ 9 44 17  2  0  0]\n",
      " [ 4 23 48 15  0  2]\n",
      " [ 0  0  7 41 15  7]\n",
      " [ 0  0  0 24 30 17]\n",
      " [ 0  0  2 12 17 53]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "#first maybe we need to train the model with the best hyperparameters on a 20% validation set\n",
    "# Split data into training and validation sets\n",
    "train_sentences, val_sentences, train_labels, val_labels = train_test_split(\n",
    "    df['sentence'], y_encoded, test_size=0.1, random_state=100)\n",
    "\n",
    "train_dataset = CEFRDataset(train_sentences, tokenizer, max_len=128, labels=train_labels)\n",
    "val_dataset = CEFRDataset(val_sentences, tokenizer, max_len=128, labels=val_labels)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)\n",
    "model = CamembertForSequenceClassification.from_pretrained('camembert-base', num_labels=len(np.unique(y_encoded)))\n",
    "model.to(device)\n",
    "optimizer = AdamW(model.parameters(), lr=best_lr)\n",
    "\n",
    "model.train()\n",
    "for epoch in range(num_epochs):\n",
    "    for batch in train_loader:\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "model.eval()\n",
    "true_labels = []\n",
    "predicted_labels = []\n",
    "for batch in val_loader:\n",
    "    batch = {k: v.to(device) for k, v in batch.items()}\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**batch)\n",
    "    logits = outputs.logits\n",
    "    predictions = torch.argmax(logits, dim=-1)\n",
    "    true_labels.extend(batch['labels'].cpu().numpy())\n",
    "    predicted_labels.extend(predictions.cpu().numpy())\n",
    "\n",
    "print(classification_report(true_labels, predicted_labels))\n",
    "print(confusion_matrix(true_labels, predicted_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#todo : train on the full dataset and save the model!!!\n",
    "full_dataset = CEFRDataset(df['sentence'], tokenizer, max_len=128, labels=y_encoded)\n",
    "full_loader = DataLoader(full_dataset, batch_size=16, shuffle=True)\n",
    "optimizer = AdamW(model.parameters(), lr=best_lr)\n",
    "num_training_steps = num_epochs * len(full_loader)\n",
    "lr_scheduler = get_scheduler(\"linear\", optimizer=optimizer, num_warmup_steps=0, num_training_steps=num_training_steps)\n",
    "\n",
    "model.train()\n",
    "for epoch in range(num_epochs):\n",
    "    for batch in tqdm.tqdm(full_loader, desc=f\"Training Epoch {epoch+1}\"):\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        optimizer.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('/Users/vaienti/Library/CloudStorage/OneDrive-epfl.ch/2024_courses/data_science_and_machine_learning/models/tokenizer_config.json',\n",
       " '/Users/vaienti/Library/CloudStorage/OneDrive-epfl.ch/2024_courses/data_science_and_machine_learning/models/special_tokens_map.json',\n",
       " '/Users/vaienti/Library/CloudStorage/OneDrive-epfl.ch/2024_courses/data_science_and_machine_learning/models/sentencepiece.bpe.model',\n",
       " '/Users/vaienti/Library/CloudStorage/OneDrive-epfl.ch/2024_courses/data_science_and_machine_learning/models/added_tokens.json')"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_path = \"/Users/vaienti/Library/CloudStorage/OneDrive-epfl.ch/2024_courses/data_science_and_machine_learning/models/\"\n",
    "model.save_pretrained(model_path)\n",
    "tokenizer.save_pretrained(model_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/vaienti/miniforge3/envs/cartogenealogy/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2688: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference complete. Results saved to 'test_with_predictions.csv'.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Assuming the model and tokenizer are already loaded and set up\n",
    "\n",
    "# Load the test dataset\n",
    "test = pd.read_csv('../test/unlabelled_test_data.csv')\n",
    "test_sentences = test['sentence'].reset_index(drop=True)\n",
    "test_dataset = CEFRDataset(test_sentences, tokenizer, max_len=128)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "# Run inference\n",
    "predictions = []\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        \n",
    "        outputs = model(input_ids, attention_mask=attention_mask)\n",
    "        logits = outputs.logits\n",
    "        pred = torch.argmax(logits, dim=1)\n",
    "        \n",
    "        predictions.extend(pred.cpu().numpy())\n",
    "\n",
    "# Decode predictions\n",
    "decoded_predictions = encoder.inverse_transform(predictions)\n",
    "\n",
    "# Add decoded predictions to the DataFrame\n",
    "test['predicted_labels'] = decoded_predictions\n",
    "import copy\n",
    "test_results = copy.deepcopy(test)\n",
    "test_results = test_results.drop(columns=['sentence'])\n",
    "test_results = test_results.rename(columns={'predicted_labels': 'difficulty'})\n",
    "test_results.to_csv('../kaggle_submissions/test_with_predictions.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Lemmatization of the sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create tokenizer and model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Separate the data into train and test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Some weights of CamembertForSequenceClassification were not initialized from the model checkpoint at camembert-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import CamembertTokenizer, CamembertForSequenceClassification, FlaubertTokenizer, FlaubertForSequenceClassification\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch\n",
    "# 1) tokenization > used to encode the sentences\n",
    "# we could do the tokenization either with Camembert or Flaubert\n",
    "# 2) padding > used to make all the sentences of the same length\n",
    "# 3) attention masks > to give the same weight to all the words, regardless of their length\n",
    "num_classes = df['difficulty'].nunique()\n",
    "chosen_tokenizer = 'camembert'\n",
    "\n",
    "if chosen_tokenizer == 'camembert':\n",
    "    tokenizer = CamembertTokenizer.from_pretrained('camembert-base')\n",
    "    model = CamembertForSequenceClassification.from_pretrained('camembert-base', num_labels=num_classes)\n",
    "elif chosen_tokenizer == 'flaubert':\n",
    "    tokenizer = FlaubertTokenizer.from_pretrained('flaubert/flaubert_base_cased')\n",
    "    model = FlaubertForSequenceClassification.from_pretrained('flaubert/flaubert_base_cased', num_labels=num_classes)\n",
    "\n",
    "tokenized = X_train.apply((lambda x_: tokenizer.encode(x_, add_special_tokens=True)))\n",
    "max_len = 0\n",
    "for i in tokenized.values:\n",
    "    if len(i) > max_len:\n",
    "        max_len = len(i)\n",
    "padded = np.array([i + [0]*(max_len-len(i)) for i in tokenized.values])\n",
    "\n",
    "attention_mask = np.where(padded != 0, 1, 0)\n",
    "# now we load the data into a torch dataloader \n",
    "# respecting the input expected by the BERT model\n",
    "\n",
    "input_ids = torch.tensor(padded)\n",
    "#create the attention mask copying with sourceTensor.clone()\n",
    "attention_mask_tensor = torch.tensor(attention_mask)\n",
    "labels = torch.tensor(y_train)\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "dataset = TensorDataset(input_ids, attention_mask_tensor, labels)\n",
    "dataloader = DataLoader(dataset, batch_size=32)\n",
    "from transformers import  get_linear_schedule_with_warmup\n",
    "from torch.optim import AdamW\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=1e-5, eps=1e-8)\n",
    "\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=len(dataloader)*epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1\n",
      "Training loss: 1.6941598991552989\n"
     ]
    }
   ],
   "source": [
    "# training loop\n",
    "from tqdm import tqdm\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)\n",
    "epochs = 1\n",
    "for epoch in range(1, epochs+1):\n",
    "    model.train()\n",
    "    loss_train_total = 0\n",
    "    progress_bar = tqdm(dataloader, desc='Epoch {:1d}'.format(epoch), leave=False, disable=False)\n",
    "    for batch in progress_bar:\n",
    "        model.zero_grad()\n",
    "        batch = tuple(b.to(device) for b in batch)\n",
    "        inputs = {'input_ids': batch[0], 'attention_mask': batch[1], 'labels': batch[2]}\n",
    "        outputs = model(**inputs)\n",
    "        loss = outputs.loss\n",
    "        loss_train_total += loss.item()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        progress_bar.set_postfix({'training_loss': '{:.3f}'.format(loss.item() / len(batch[0]))})\n",
    "    torch.save(model.state_dict(), f'BERT_ft_epoch{epoch}.model')\n",
    "    tqdm.write(f'\\nEpoch {epoch}')\n",
    "    loss_train_avg = loss_train_total/len(dataloader)\n",
    "    tqdm.write(f'Training loss: {loss_train_avg}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predicting Levels\n",
    "We can now use our model to predict the level of a text. To do this, we need to correctly encode our text in the same way as our data was encoded during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/960 [00:00<?, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/Users/vaienti/miniforge3/envs/cartogenealogy/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2688: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "100%|██████████| 960/960 [01:28<00:00, 10.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          A1       0.42      0.94      0.58       166\n",
      "          A2       0.29      0.15      0.19       158\n",
      "          B1       0.42      0.10      0.17       166\n",
      "          B2       0.44      0.12      0.19       153\n",
      "          C1       0.29      0.11      0.16       152\n",
      "          C2       0.37      0.83      0.51       165\n",
      "\n",
      "    accuracy                           0.38       960\n",
      "   macro avg       0.37      0.37      0.30       960\n",
      "weighted avg       0.37      0.38      0.30       960\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def predict_text(text, device):\n",
    "    encoded_text = tokenizer.encode_plus(\n",
    "        text,\n",
    "        max_length=128,\n",
    "        add_special_tokens=True,\n",
    "        return_token_type_ids=False,\n",
    "        pad_to_max_length=True,\n",
    "        return_attention_mask=True,\n",
    "        return_tensors='pt',\n",
    "    )\n",
    "    input_ids = encoded_text['input_ids'].to(device)\n",
    "    attention_mask = encoded_text['attention_mask'].to(device)\n",
    "    output = model(input_ids, attention_mask)\n",
    "    _, prediction = torch.max(output[0], dim=1)\n",
    "    return prediction[0].item()\n",
    "\n",
    "y_pred = []\n",
    "for text in tqdm(X_test):\n",
    "    y_pred.append(predict_text(text, device))\n",
    "    \n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test, y_pred, target_names=['A1', 'A2', 'B1', 'B2', 'C1', 'C2']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Cartogenealogy",
   "language": "python",
   "name": "cartogenealogy"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
