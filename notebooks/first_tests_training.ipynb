{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "#we read the training data\n",
    "df = pd.read_csv('training_data.csv')\n",
    "#read the test data\n",
    "\n",
    "baserate = df['difficulty'].value_counts() / len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label mapping: {'A1': 0, 'A2': 1, 'B1': 2, 'B2': 3, 'C1': 4, 'C2': 5}\n"
     ]
    }
   ],
   "source": [
    "# Drop rows with missing 'sentence' or 'difficulty' in training data\n",
    "df = df.dropna(subset=['sentence', 'difficulty'])\n",
    "\n",
    "# Fill or drop missing 'sentence' in test data\n",
    "#test = test.dropna(subset=['sentence'])  # Assuming you want to drop\n",
    "\n",
    "# Optionally, remove duplicates\n",
    "df = df.drop_duplicates(subset=['sentence'])\n",
    "#test = test.drop_duplicates(subset=['sentence'])\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re\n",
    "\n",
    "# Initialize lemmatizer (we use spacy because it has french lemmatization)\n",
    "\n",
    "import spacy\n",
    "\n",
    "# Load the French Spacy model\n",
    "nlp = spacy.load('fr_core_news_md')\n",
    "\n",
    "def preprocess_text(text):\n",
    "    doc = nlp(text.lower())\n",
    "    result = []\n",
    "    for token in doc:\n",
    "        if not token.is_stop and not token.is_punct:\n",
    "            result.append(token.lemma_)\n",
    "    return ' '.join(result)\n",
    "\n",
    "# Apply preprocessing to the sentence columns\n",
    "df['sentence'] = df['sentence'].apply(preprocess_text)\n",
    "\n",
    "\n",
    "# Load pre-trained BERT model and tokenizer\n",
    "# Initialize BERT tokenizer and model\n",
    "from transformers import CamembertTokenizer, CamembertModel\n",
    "import torch\n",
    "\n",
    "# Load pre-trained CamemBERT model and tokenizer\n",
    "tokenizer = CamembertTokenizer.from_pretrained('camembert-base')\n",
    "camembert_model = CamembertModel.from_pretrained('camembert-base')\n",
    "\n",
    "def get_camembert_embedding(sentence):\n",
    "    # Prepare the text input for CamemBERT\n",
    "    inputs = tokenizer(sentence, return_tensors=\"pt\", padding=True, truncation=True, max_length=128)\n",
    "    # Get output from CamemBERT model\n",
    "    with torch.no_grad():\n",
    "        outputs = camembert_model(**inputs)\n",
    "    # Extract the mean of the last hidden state to use as the sentence embedding\n",
    "    embeddings = outputs.last_hidden_state.mean(dim=1).squeeze()\n",
    "    return embeddings.detach().cpu().numpy()\n",
    "\n",
    "# Example of processing a batch of sentences\n",
    "def get_camembert_embeddings(sentences, batch_size=32):\n",
    "    embeddings = []\n",
    "    for i in range(0, len(sentences), batch_size):\n",
    "        batch = sentences[i:i + batch_size]\n",
    "        inputs = tokenizer(batch, return_tensors=\"pt\", padding=True, truncation=True, max_length=128)\n",
    "        with torch.no_grad():\n",
    "            outputs = camembert_model(**inputs)\n",
    "        batch_embeddings = outputs.last_hidden_state.mean(dim=1)\n",
    "        embeddings.append(batch_embeddings.detach().cpu().numpy())\n",
    "    return np.vstack(embeddings)\n",
    "\n",
    "# Process embeddings in batches\n",
    "embeddings = get_camembert_embeddings(df['sentence'].tolist())\n",
    "df['embedding'] = list(embeddings)\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "X = np.array(df['embedding'].tolist())\n",
    "y = df['difficulty'].values\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Define the order of your labels\n",
    "labels_ordered = ['A1', 'A2', 'B1', 'B2', 'C1', 'C2']\n",
    "\n",
    "# Initialize the LabelEncoder\n",
    "encoder = LabelEncoder()\n",
    "\n",
    "# Manually fit the encoder to the ordered labels\n",
    "encoder.fit(labels_ordered)\n",
    "\n",
    "# Encode your actual labels\n",
    "y_encoded = encoder.transform(y)\n",
    "\n",
    "# Output the encoding to verify\n",
    "label_mapping = dict(zip(encoder.classes_, encoder.transform(encoder.classes_)))\n",
    "print(\"Label mapping:\", label_mapping)\n",
    "\n",
    "\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "class TextDifficultyNN(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(TextDifficultyNN, self).__init__()\n",
    "        self.layer1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.layer2 = nn.Linear(hidden_dim, output_dim)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layer1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.softmax(x)\n",
    "        return x\n",
    "# Assuming you have already determined the number of classes\n",
    "num_classes = len(set(y))  # Replace this with the actual number of difficulty classes\n",
    "input_dim = 768  # Size of the CamemBERT embedding\n",
    "hidden_dim = 100  # You can tune this\n",
    "output_dim = num_classes\n",
    "\n",
    "model = TextDifficultyNN(input_dim, hidden_dim, output_dim)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7919 with hidden_dim=150, lr=0.001, batch_size=8\n",
      "Accuracy: 0.8089 with hidden_dim=200, lr=0.001, batch_size=8\n",
      "Accuracy: 0.8123 with hidden_dim=250, lr=0.001, batch_size=8\n",
      "Best Parameters: {'hidden_dim': 250, 'lr': 0.001, 'batch_size': 8}\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "def train_and_evaluate_model(X, y_encoded, input_dim, hidden_dim, output_dim, lr, batch_size, num_epochs):\n",
    "    # Convert data to tensors inside the function if not already tensors\n",
    "    X_tensor = torch.tensor(X, dtype=torch.float32)\n",
    "    y_tensor = torch.tensor(y_encoded, dtype=torch.long)\n",
    "\n",
    "    # Model setup\n",
    "    model = TextDifficultyNN(input_dim, hidden_dim, output_dim)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    # Track performance\n",
    "    accuracies = []\n",
    "\n",
    "    # K-fold cross-validation\n",
    "    for train_idx, val_idx in kf.split(X):\n",
    "        train_dataset = TensorDataset(X_tensor[train_idx], y_tensor[train_idx])\n",
    "        val_dataset = TensorDataset(X_tensor[val_idx], y_tensor[val_idx])\n",
    "\n",
    "        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "        val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "        # Training phase\n",
    "        for epoch in range(num_epochs):\n",
    "            model.train()\n",
    "            for inputs, labels in train_loader:\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "        # Evaluation phase\n",
    "        model.eval()\n",
    "        correct = total = 0\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in val_loader:\n",
    "                outputs = model(inputs)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "        accuracy = correct / total\n",
    "        accuracies.append(accuracy)\n",
    "\n",
    "    return np.mean(accuracies)\n",
    "\n",
    "# Hyperparameter grid\n",
    "hidden_dims = [150, 200, 250]\n",
    "learning_rates = [0.001]\n",
    "batch_sizes = [8]\n",
    "num_epochs = 30\n",
    "\n",
    "best_accuracy = 0\n",
    "best_params = {}\n",
    "\n",
    "for hidden_dim in hidden_dims:\n",
    "    for lr in learning_rates:\n",
    "        for batch_size in batch_sizes:\n",
    "            accuracy = train_and_evaluate_model(X, y_encoded, input_dim, hidden_dim, output_dim, lr, batch_size, num_epochs)\n",
    "            print(f\"Accuracy: {accuracy:.4f} with hidden_dim={hidden_dim}, lr={lr}, batch_size={batch_size}\")\n",
    "            if accuracy > best_accuracy:\n",
    "                best_accuracy = accuracy\n",
    "                best_params = {'hidden_dim': hidden_dim, 'lr': lr, 'batch_size': batch_size}\n",
    "\n",
    "print(\"Best Parameters:\", best_params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 1.2850\n",
      "Epoch 11, Loss: 0.4933\n",
      "Epoch 21, Loss: 0.2340\n",
      "Epoch 31, Loss: 0.2033\n",
      "Epoch 41, Loss: 0.0405\n",
      "Epoch 51, Loss: 0.0048\n",
      "Epoch 61, Loss: 0.0174\n",
      "Epoch 71, Loss: 0.0046\n"
     ]
    }
   ],
   "source": [
    "# Initialize the model with these best parameters\n",
    "\n",
    "\n",
    "model = TextDifficultyNN(input_dim=768, hidden_dim=best_params['hidden_dim'], output_dim=len(encoder.classes_))\n",
    "optimizer = optim.Adam(model.parameters(), lr=best_params['lr'])\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Convert the entire dataset to tensors\n",
    "X_tensor_full = torch.tensor(X, dtype=torch.float32)\n",
    "y_tensor_full = torch.tensor(y_encoded, dtype=torch.long)\n",
    "\n",
    "full_dataset = TensorDataset(X_tensor_full, y_tensor_full)\n",
    "full_loader = DataLoader(full_dataset, batch_size=best_params['batch_size'], shuffle=True)\n",
    "\n",
    "# Train the model\n",
    "num_epochs = 80  # Adjust based on the training convergence observed in tuning\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for inputs, labels in full_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    if epoch % 10 == 0:  # Print loss every 10 epochs\n",
    "        print(f'Epoch {epoch+1}, Loss: {loss.item():.4f}')\n",
    "\n",
    "# Assuming test_embeddings is already prepared and is a numpy array\n",
    "test = pd.read_csv('unlabelled_test_data.csv')\n",
    "test['sentence'] = test['sentence'].apply(preprocess_text)\n",
    "test_embeddings = get_camembert_embeddings(test['sentence'].tolist())\n",
    "X_test_tensor = torch.tensor(test_embeddings, dtype=torch.float32)\n",
    "test_loader = DataLoader(X_test_tensor, batch_size=best_params['batch_size'], shuffle=False)\n",
    "\n",
    "model.eval()  # Set the model to evaluation mode\n",
    "predictions = []\n",
    "with torch.no_grad():\n",
    "    for inputs in test_loader:\n",
    "        outputs = model(inputs)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        predictions.extend(predicted.cpu().numpy())\n",
    "\n",
    "# Decode the predictions back to labels\n",
    "predicted_labels = encoder.inverse_transform(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions saved to 'final_predictions.csv'.\n",
      "Model saved to 'final_trained_model.pth'.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assume you have a DataFrame 'test_df' that corresponds to the test embeddings\n",
    "test['difficulty'] = predicted_labels\n",
    "#drop the sentence and embedding columns\n",
    "test_df = test.drop(columns=['sentence'])\n",
    "# Save predictions to a CSV file\n",
    "test_df.to_csv('final_predictions_camembert_nn.csv', index=False)\n",
    "print(\"Predictions saved to 'final_predictions.csv'.\")\n",
    "\n",
    "torch.save(model.state_dict(), 'final_trained_model.pth')\n",
    "print(\"Model saved to 'final_trained_model.pth'.\")\n",
    "\n",
    "#the final results quality are lower than the evaluation so it is overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean accuracy across all folds: 0.34812499999999996\n"
     ]
    }
   ],
   "source": [
    "#option A  TF-IDF Vectorization\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Example: Using TF-IDF Vectorization\n",
    "vectorizer = TfidfVectorizer(max_features=1000)  # Adjust the number of features as needed\n",
    "X = vectorizer.fit_transform(df['sentence'])\n",
    "y = df['difficulty'].values\n",
    "\n",
    "# Initialize the KFold method\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)  # You can choose the number of splits\n",
    "\n",
    "# Initialize a classifier, e.g., RandomForest\n",
    "model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "# To store the fold scores\n",
    "scores = []\n",
    "\n",
    "# Execute k-fold cross-validation\n",
    "for train_index, test_index in kf.split(X):\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "    # Train the model\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # Predict on the test fold\n",
    "    predictions = model.predict(X_test)\n",
    "\n",
    "    # Calculate accuracy or other metrics\n",
    "    score = accuracy_score(y_test, predictions)\n",
    "    scores.append(score)\n",
    "\n",
    "# Print out the mean accuracy across all folds\n",
    "print(\"Mean accuracy across all folds:\", np.mean(scores))\n",
    "\n",
    "# Retrain on the entire training set\n",
    "model.fit(X, y)\n",
    "\n",
    "# Prepare the test set (Assuming you have preprocessed it as well)\n",
    "X_final_test = vectorizer.transform(test['sentence'])\n",
    "\n",
    "# Predict on the unlabelled test set\n",
    "final_predictions = model.predict(X_final_test)\n",
    "\n",
    "# Optionally, save or return your predictions\n",
    "test['predicted_difficulty'] = final_predictions\n",
    "test.to_csv('final_predictions.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sentencepiece\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48b2bc8918d142f5992dcc95dfe4d601",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79bd8a25df1b41b5a3f4cc78a0e2851f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentencepiece.bpe.model:   0%|          | 0.00/811k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ceee67425f804ec48966d2aff3c25255",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.40M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "513287fb006841b4865d8388a554587f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/508 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9e0a758e79b4e8085cea954034b393c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/445M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean accuracy across all folds: 0.43520833333333336\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hyperparameters:\n",
    "# we need to try our hyperparameters in a sort of validation set (we do a kfold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "in the report we should fill the table (the report is the readme of the github)\n",
    "\n",
    "it's more useful to play on the choice of the technique and on the choice of the hyperparameters (not so much on data cleaning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Cartogenealogy",
   "language": "python",
   "name": "cartogenealogy"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
